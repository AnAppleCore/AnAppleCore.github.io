pub_date	title	venue	excerpt	citation	url_slug	paper_url
2024-04-02	Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation	IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024	This paper is about our proposed online continual learning algorithm MOSE, which is inspired by the multi-level feature extraction and cross-layer communication inherent to animal neural circuits, aiming to enhance the model's adaptivity to dynamic distributions and resistance against forgetting.	Hongwei Yan, Liyuan Wang, Kaisheng Ma, and Yi Zhong. "Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation." <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024</i>	mose	https://arxiv.org/pdf/2404.00417


2025-04-11	MindPainter: Efficient Brain-Conditioned Painting of Natural Images via Cross-Modal Self-Supervised Learning	The 39th Annual AAAI Conference on Artificial Intelligence 2024	This paper proposes MindPainter, a method for efficient brain-conditioned image editing using brain signals of visual perception as prompts. By employing cross-modal self-supervised learning, it directly reconstructs masked images with pseudo-brain signals generated by the Pseudo Brain Generator, enabling seamless cross-modal integration. The Brain Adapter ensures accurate interpretation of brain signals, while the Multi-Mask Generation Policy enhances generalization for high-quality editing in various scenarios, such as inpainting and outpainting. MindPainter is the first to achieve efficient brain-conditioned image painting, advancing direct brain control in creative AI.	Muzhou Yu, Shuyun Lin, Hongwei Yan, Kaisheng Ma. "MindPainter: Efficient Brain-Conditioned Painting of Natural Images via Cross-Modal Self-Supervised Learning." <i>The 39th Annual AAAI Conference on Artificial Intelligence 2024</i>	mindpainter	https://ojs.aaai.org/index.php/AAAI/article/view/33585


2025-05-01	Right Time to Learn: Promoting Generalization via Bio-inspired Spacing Effect in Knowledge Distillation	The 42nd International Conference on Machine Learning 2025	Knowledge distillation (KD) is a powerful strategy for training deep neural networks (DNNs). Although it was originally proposed to train a more compact student model from a large teacher model, many recent efforts have focused on adapting it to promote generalization of the model itself, such as online KD and self KD. % as an effective way Here, we propose an accessible and compatible strategy named Spaced KD to improve the effectiveness of both online KD and self KD, in which the student model distills knowledge from a teacher model trained with a space interval ahead. This strategy is inspired by a prominent theory named \emph{spacing effect} in biological learning and memory, positing that appropriate intervals between learning trials can significantly enhance learning performance. With both theoretical and empirical analyses, we demonstrate that the benefits of the proposed Spaced KD stem from convergence to a flatter loss landscape during stochastic gradient descent (SGD). We perform extensive experiments to validate the effectiveness of Spaced KD in improving the learning performance of DNNs (e.g., the performance gain is up to 2.31\% and 3.34\% on Tiny-ImageNet over online KD and self KD, respectively).	Guanglong Sun*, Hongwei Yan*, Liyuan Wang, Qian Li, Bo Lei, Yi Zhong. "Right Time to Learn: Promoting Generalization via Bio-inspired Spacing Effect in Knowledge Distillation." <i>The 42nd International Conference on Machine Learning 2025</i>	spacedkd	https://arxiv.org/abs/2502.06192