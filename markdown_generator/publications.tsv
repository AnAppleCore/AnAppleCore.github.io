pub_date	title	venue	excerpt	citation	url_slug	paper_url
2024-04-02	Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation	IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024	This paper is about our proposed online continual learning algorithm MOSE, which is inspired by the multi-level feature extraction and cross-layer communication inherent to animal neural circuits, aiming to enhance the model's adaptivity to dynamic distributions and resistance against forgetting.	<strong>Hongwei Yan</strong>, Liyuan Wang*, Kaisheng Ma*, and Yi Zhong*. "Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation." <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024</i>	mose	https://arxiv.org/pdf/2404.00417


2024-04-02	MindPainter: Efficient Brain-Conditioned Painting of Natural Images via Cross-Modal Self-Supervised Learning	The 39th Annual AAAI Conference on Artificial Intelligence 2024	This paper proposes MindPainter, a method for efficient brain-conditioned image editing using brain signals of visual perception as prompts. By employing cross-modal self-supervised learning, it directly reconstructs masked images with pseudo-brain signals generated by the Pseudo Brain Generator, enabling seamless cross-modal integration. The Brain Adapter ensures accurate interpretation of brain signals, while the Multi-Mask Generation Policy enhances generalization for high-quality editing in various scenarios, such as inpainting and outpainting. MindPainter is the first to achieve efficient brain-conditioned image painting, advancing direct brain control in creative AI.	Muzhou Yu, Shuyun Lin, <strong>Hongwei Yan</strong>, Kaisheng Ma*. "MindPainter: Efficient Brain-Conditioned Painting of Natural Images via Cross-Modal Self-Supervised Learning." <i>The 39th Annual AAAI Conference on Artificial Intelligence 2024</i>	mindpainter	https://arxiv.org/pdf/xxxx.xxxxx