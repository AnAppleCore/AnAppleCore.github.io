---
title: "Right Time to Learn: Promoting Generalization via Bio-inspired Spacing Effect in Knowledge Distillation"
collection: publications
permalink: /publication/2025-05-01-spacedkd
excerpt: 'Knowledge distillation (KD) is a powerful strategy for training deep neural networks (DNNs). Although it was originally proposed to train a more compact student model from a large teacher model, many recent efforts have focused on adapting it to promote generalization of the model itself, such as online KD and self KD. % as an effective way Here, we propose an accessible and compatible strategy named Spaced KD to improve the effectiveness of both online KD and self KD, in which the student model distills knowledge from a teacher model trained with a space interval ahead. This strategy is inspired by a prominent theory named \emph{spacing effect} in biological learning and memory, positing that appropriate intervals between learning trials can significantly enhance learning performance. With both theoretical and empirical analyses, we demonstrate that the benefits of the proposed Spaced KD stem from convergence to a flatter loss landscape during stochastic gradient descent (SGD). We perform extensive experiments to validate the effectiveness of Spaced KD in improving the learning performance of DNNs (e.g., the performance gain is up to 2.31\% and 3.34\% on Tiny-ImageNet over online KD and self KD, respectively).'
date: 2025-05-01
venue: 'The 42nd International Conference on Machine Learning 2025'
paperurl: 'https://arxiv.org/abs/2502.06192'
citation: 'Guanglong Sun*, Hongwei Yan*, Liyuan Wang, Qian Li, Bo Lei, Yi Zhong. &quot;Right Time to Learn: Promoting Generalization via Bio-inspired Spacing Effect in Knowledge Distillation.&quot; <i>The 42nd International Conference on Machine Learning 2025</i>'
---
Knowledge distillation (KD) is a powerful strategy for training deep neural networks (DNNs). Although it was originally proposed to train a more compact student model from a large teacher model, many recent efforts have focused on adapting it to promote generalization of the model itself, such as online KD and self KD. % as an effective way Here, we propose an accessible and compatible strategy named Spaced KD to improve the effectiveness of both online KD and self KD, in which the student model distills knowledge from a teacher model trained with a space interval ahead. This strategy is inspired by a prominent theory named \emph{spacing effect} in biological learning and memory, positing that appropriate intervals between learning trials can significantly enhance learning performance. With both theoretical and empirical analyses, we demonstrate that the benefits of the proposed Spaced KD stem from convergence to a flatter loss landscape during stochastic gradient descent (SGD). We perform extensive experiments to validate the effectiveness of Spaced KD in improving the learning performance of DNNs (e.g., the performance gain is up to 2.31\% and 3.34\% on Tiny-ImageNet over online KD and self KD, respectively).

[Download paper here](https://arxiv.org/abs/2502.06192)

Recommended citation: Guanglong Sun*, Hongwei Yan*, Liyuan Wang, Qian Li, Bo Lei, Yi Zhong. "Right Time to Learn: Promoting Generalization via Bio-inspired Spacing Effect in Knowledge Distillation." <i>The 42nd International Conference on Machine Learning 2025</i>