---
title: "Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation"
collection: publications
permalink: /publication/2024-04-02-mose
excerpt: 'This paper is about our proposed online continual learning algorithm MOSE, which is inspired by the multi-level feature extraction and cross-layer communication inherent to animal neural circuits, aiming to enhance the model&apos;s adaptivity to dynamic distributions and resistance against forgetting.'
date: 2024-04-02
venue: 'IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024'
paperurl: 'https://arxiv.org/pdf/2404.00417'
citation: 'Hongwei Yan, Liyuan Wang*, Kaisheng Ma*, and Yi Zhong*. &quot;Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation.&quot; <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024</i>'
---

<a href='https://arxiv.org/pdf/2404.00417'>Download paper here</a>

This paper is about our proposed online continual learning algorithm MOSE, which is inspired by the multi-level feature extraction and cross-layer communication inherent to animal neural circuits, aiming to enhance the model&apos;s adaptivity to dynamic distributions and resistance against forgetting.

Recommended citation: Hongwei Yan, Liyuan Wang*, Kaisheng Ma*, and Yi Zhong*. "Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation." <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024</i>