---
title: "Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation"
collection: publications
permalink: /publication/2009-10-01-paper-title-number-1
excerpt: 'This paper is about our proposed online continual learning algorithm MOSE, which is inspired by the multi-level feature extraction and cross-layer communication inherent to animal neural circuits, aiming to enhance the model's adaptivity to dynamic distributions and resistance against forgetting.'
date: 2024-04-02
venue: 'IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)'
paperurl: 'https://arxiv.org/pdf/2404.00417'
citation: 'Hongwei Yan, Liyuan Wang, Kaisheng Ma, and Yi Zhong. (2024). &quot;Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation.&quot; <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>. 1(1).'
---
This paper is about our proposed online continual learning algorithm MOSE, which is inspired by the multi-level feature extraction and cross-layer communication inherent to animal neural circuits, aiming to enhance the model's adaptivity to dynamic distributions and resistance against forgetting.

[Download paper here](https://arxiv.org/pdf/2404.00417)

Hongwei Yan, Liyuan Wang, Kaisheng Ma, and Yi Zhong. (2024). &quot;Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation.&quot; <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>. 1(1).